{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361ed215-9b26-48e7-927e-fa2abbcfa2bb",
   "metadata": {},
   "source": [
    "# Keras Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377832a-4f40-41e0-9c93-45f413fcc0f4",
   "metadata": {},
   "source": [
    "In this notebook, we explored the best parameters for the dense layers using the Hyperband tuning algorithm implemented in Keras. However, we did not use these predictions as they did not perform as expected during the fine-tuning phase. \n",
    "\n",
    "We also made a mistake by not exploring the HeUniform initialization distribution strategy for ReLU dense layers. Instead, we only explored the Glorot Uniform, which is more suitable for sigmoid activation layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f34aad-199b-48fa-9546-88f439c8fefa",
   "metadata": {},
   "source": [
    "The keras tuner here uses the Hyperband algorithm, and to implement it we followed the implementation provided in https://keras.io/api/keras_tuner/tuners/hyperband/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818fa53-0066-441f-ae88-6aaa561d9a6e",
   "metadata": {},
   "source": [
    "## Tuning Dense Layers of ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4877340-fb67-4875-9b11-fa821bee4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers, initializers\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def model_builder(hp):\n",
    "        \n",
    "    input = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    \n",
    "    x = tf.keras.layers.Resizing(96, 96, interpolation='bicubic', name='resizing')(input)\n",
    "    \n",
    "    # Add data augmentation layer here\n",
    "    x = data_augmentation(x)  \n",
    "    x = model_cnn(x)  \n",
    "    \n",
    "    # Use a hyperparameter to decide.\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.5, step = 0.05)   \n",
    "    \n",
    "    #Choice between Globalaverage pooling and Flatten.\n",
    "    #In case it is Flatten, we also include the dropout\n",
    "    \n",
    "    if hp.Choice('pooling', ['flatten', 'global']) == 'flatten':\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dropout(hp_dropout)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add a hyperparameter for the regularization type\n",
    "    hp_reg_type = hp.Choice('reg_type', ['l1', 'l2'])\n",
    "    \n",
    "    # Add a hyperparameter for the regularization strength\n",
    "    hp_reg_strength = hp.Float('reg_strength', min_value=0.0, max_value=0.1, step=0.02)\n",
    "    \n",
    "    # Add a hyperparameter for the initialization distribution\n",
    "    hp_init_distribution = hp.Choice('init_distribution', [ 'glorot_uniform', 'glorot_normal'])\n",
    "    \n",
    "    \n",
    "    for i in range(hp.Int('n_layers',1,3)):\n",
    "        if hp_reg_type == 'l1':\n",
    "            reg = regularizers.l1(hp_reg_strength)\n",
    "        else:\n",
    "            reg = regularizers.l2(hp_reg_strength)\n",
    "        \n",
    "        if hp_init_distribution == 'uniform':\n",
    "            init = initializers.RandomUniform(seed=SEED)\n",
    "            \n",
    "        elif hp_init_distribution == 'normal':\n",
    "            init = initializers.RandomNormal(seed=SEED)\n",
    "            \n",
    "        elif hp_init_distribution == 'glorot_uniform':\n",
    "            init = initializers.GlorotUniform(seed=SEED)\n",
    "        else:\n",
    "            init = initializers.GlorotNormal(seed=SEED)\n",
    "        \n",
    "        #Add  variable number of dense layers, from 128 to 1024\n",
    "        x = layers.Dense(units=hp.Int(f'units_{i}', min_value=128, max_value=1024, step=128),\n",
    "                         kernel_regularizer=reg, kernel_initializer=init)(x)\n",
    "        \n",
    "        #In this case, we add batch normalization, then use relu as activation\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        #The dropout will depend on the tuner\n",
    "        x = layers.Dropout(hp_dropout)(x)       \n",
    "    \n",
    "    #The last layer will always be a dense layer with sigmoid activation\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=init)(x)\n",
    "\n",
    "    model = keras.Model(input, outputs)\n",
    "\n",
    "    # best learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate',values=[1e-2,1e-3,1e-4])\n",
    "    \n",
    "    # Define the hyperparameter for the optimizer\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
    "    \n",
    "    #Always use \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=hp_optimizer, metrics=['binary_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', \n",
    "                                              patience=10)\n",
    "\n",
    "tuner = kt.Hyperband(model_builder, \n",
    "                     objective='val_binary_accuracy', \n",
    "                     directory=\"Tuner Results\") \n",
    "\n",
    "tuner.search(training_set, epochs = 20, \n",
    "             validation_data = validation_set, \n",
    "             class_weight = class_weights_dict,\n",
    "             callbacks=[stop_early])     \n",
    "\n",
    "# Get the best model  \n",
    "models = tuner.get_best_models(num_models=1)  \n",
    "model = models[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f702a22-0d91-49f4-908b-6f61ceed5a76",
   "metadata": {},
   "source": [
    "Best hyperparameters:\n",
    "- Activation: leaky_relu\n",
    "- Dropout: 0.4\n",
    "- Pooling: global\n",
    "- Regularization Type: l1\n",
    "- Regularization Strength: 0.0\n",
    "- Initialization Distribution: glorot_uniform\n",
    "- Number of Layers: 3\n",
    "- Learning Rate: 0.0001\n",
    "- Optimizer: adam\n",
    "\n",
    "Units in layer 1:128\n",
    "Units in layer 2:1024\n",
    "Units in Layer 3:768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf897483-dd46-4f4d-aeb8-b0cf35c62184",
   "metadata": {},
   "source": [
    "## Tuning Dense Layers of EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084d1a0-aec5-43fc-a542-42d0f360df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers, initializers\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def model_builder(hp):\n",
    "        \n",
    "    input = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    x = tf.keras.layers.Resizing(96, 96, interpolation='bicubic', name='resizing')(input)\n",
    "    \n",
    "    x = data_augmentation(x)  # Add data augmentation layer here\n",
    "    x = model_cnn(x)    \n",
    "    \n",
    "    # Use a hyperparameter to decide.\n",
    "    hp_activation = hp.Choice('activation', values = ['relu', 'swish','gelu', 'leaky_relu'])\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.5, step = 0.05)   \n",
    "    if hp.Choice('pooling', ['flatten', 'global']) == 'flatten':\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dropout(hp_dropout)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add a hyperparameter for the regularization type\n",
    "    hp_reg_type = hp.Choice('reg_type', ['l1', 'l2'])\n",
    "    \n",
    "    # Add a hyperparameter for the regularization strength\n",
    "    hp_reg_strength = hp.Float('reg_strength', min_value=0.0, max_value=0.1, step=0.01)\n",
    "    \n",
    "    # Add a hyperparameter for the initialization distribution\n",
    "    hp_init_distribution = hp.Choice('init_distribution', ['uniform', 'normal', 'glorot_uniform', 'glorot_normal'])\n",
    "    \n",
    "    for i in range(hp.Int('n_layers',1,5)):\n",
    "        if hp_reg_type == 'l1':\n",
    "            reg = regularizers.l1(hp_reg_strength)\n",
    "        else:\n",
    "            reg = regularizers.l2(hp_reg_strength)\n",
    "        \n",
    "        if hp_init_distribution == 'uniform':\n",
    "            init = initializers.RandomUniform(seed=SEED)\n",
    "        \n",
    "        elif hp_init_distribution == 'normal':\n",
    "            init = initializers.RandomNormal(seed=SEED)\n",
    "        \n",
    "        elif hp_init_distribution == 'glorot_uniform':\n",
    "            init = initializers.GlorotUniform(seed=SEED)\n",
    "        else:\n",
    "            init = initializers.GlorotNormal(seed=SEED)\n",
    "        \n",
    "        x = layers.Dense(units=hp.Int(f'units_{i}', min_value=128, max_value=1024, step=128),\n",
    "                         kernel_regularizer=reg, kernel_initializer=init)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(hp_activation)(x)\n",
    "        x = layers.Dropout(hp_dropout)(x)       \n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=init)(x)\n",
    "\n",
    "    model = keras.Model(input, outputs)\n",
    "\n",
    "    # best lr\n",
    "    hp_learning_rate = hp.Choice('learning_rate',values=[1e-2,1e-3,1e-4])\n",
    "    # Define the hyperparameter for the optimizer\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'adagrad'])\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=hp_optimizer, metrics=['binary_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=10)\n",
    "tuner = kt.Hyperband(model_builder, objective='val_binary_accuracy', directory=\"Tuner Results\",)\n",
    "tuner.search(training_set, epochs = 20, validation_data = validation_set, class_weight = class_weights_dict,callbacks=[stop_early])                  \n",
    "# Get the best model  \n",
    "models = tuner.get_best_models(num_models=1)  \n",
    "model = models[0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31af444-fa3f-441a-8471-8daa2887f920",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Best hyperparameters:\n",
    "- Activation: gelu\n",
    "- Dropout: 0.35000000000000003\n",
    "- Pooling: global\n",
    "- Regularization Type: l1\n",
    "- Regularization Strength: 0.0\n",
    "- Initialization Distribution: glorot_normal\n",
    "- Number of Layers: 1\n",
    "- Learning Rate: 0.001\n",
    "- Optimizer: rmsprop\n",
    "\n",
    "Units in the layers:\n",
    "- Units in layer 1: 768\n",
    "- Units in layer 2:0\n",
    "- Units in layer 3: 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
