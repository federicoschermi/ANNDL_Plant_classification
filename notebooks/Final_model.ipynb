{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.14\n",
    "!pip install keras-cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from shutil import copyfile, make_archive, unpack_archive #for saving the submission\n",
    "import sklearn as scikit_learn\n",
    "from datetime import datetime\n",
    "\n",
    "#For random seed\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "#Check tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'training_data_final' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting general simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Image parameters\n",
    "IMG_SIZE = (96,96)\n",
    "IMG_SHAPE = (96,96,3)\n",
    "\n",
    "#Number of classes: healthy and unhealthy\n",
    "N_CLASSES = 2\n",
    "\n",
    "#We opt for a 80-20 train-validation split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "#We set a large batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Set the random seed for generalizability\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiating dataset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    class_names = None,\n",
    "    color_mode='rgb',\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    class_names = None,\n",
    "    color_mode='rgb',\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of images: 5004.\n",
    "#### Training images: 4004.\n",
    "#### Validation images: 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of class weights\n",
    "\n",
    "#### We introduce class weights, that will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We instantiate a generator to access the data which are stored in two separate folders\n",
    "datagen_for_class_weights = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "my_data = datagen_for_class_weights.flow_from_directory(DATA_DIR)\n",
    "\n",
    "#We count the unique number of occurrences of the class\n",
    "unique = np.unique(my_data.classes, return_counts=True)\n",
    "\n",
    "#We use the scikit_learn function to compute class weights to make the class balanced (inversely proportional to the number of elements per class)\n",
    "myclass_weights = scikit_learn.utils.compute_class_weight('balanced', \n",
    "                                                          y=my_data.classes,\n",
    "                                                          classes = np.unique(my_data.classes) )\n",
    "class_weights_dict = dict(enumerate(myclass_weights.flatten()))\n",
    "\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on our approach, the class weights are approaximately 0.807, 1: 1.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce mixed precision policy to optimize memory consumption\n",
    "\n",
    "``keras.mixed_precision`` allows to adaptively use different precisions (in our case 0.16 and 0.32 floating point precision) during training, in a way that optimized memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':\n",
    "    print(\"Mixed precision is enabled.\")\n",
    "else:\n",
    "    print(\"Mixed precision is not enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce autotune for the training and validation set cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce the `autotune` function, which is used o optimally tune data. In this case, we are using it to shuffle the elements of the dataset with a buffer size of 1000. \n",
    "\n",
    "More in detail, `training_set.cache()` is used to cache the elements of the dataset. This step is done to improve the performance of data loading by keeping the data in memory. The further arguent introduces a shuffle, which is introduced to  shuffles the elements of the dataset with a buffer size of 1000 to ensure different samples are seen during each epoch. Similar reasoning can be used for `validation_set.cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:42.240165Z",
     "iopub.status.busy": "2023-11-18T17:10:42.239777Z",
     "iopub.status.idle": "2023-11-18T17:10:42.252857Z",
     "shell.execute_reply": "2023-11-18T17:10:42.252060Z",
     "shell.execute_reply.started": "2023-11-18T17:10:42.240133Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "training_set = training_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_set = validation_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the generalizability of our results, we rely on a data augmentation strategy. \n",
    "\n",
    "We introduce a series of basic transformations. For more information on the transformations which we have decided to apply. Please refer to the `augmentation_study.ipynb` notebook for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:45.106914Z",
     "iopub.status.busy": "2023-11-18T17:10:45.106535Z",
     "iopub.status.idle": "2023-11-18T17:10:45.131794Z",
     "shell.execute_reply": "2023-11-18T17:10:45.130556Z",
     "shell.execute_reply.started": "2023-11-18T17:10:45.106881Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),  # Horizontal and vertical random flip \n",
    "  tf.keras.layers.RandomBrightness(0.1),  # Random brightness shift until 10% of the intensity\n",
    "  tf.keras.layers.RandomTranslation(\n",
    "      height_factor=0.05,  # Random height shift until 5% of the height\n",
    "      width_factor=0.05,   # Random width shift until 5% of the width\n",
    "      fill_mode='reflect' # When there are holes, the closest pixels are reflected \n",
    "  ),\n",
    "tf.keras.layers.RandomRotation(0.125)  # Random rotation of +-45 degrees\n",
    "], name='data_augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model with ConvNeXtXLarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we set the hyperparameters specific to the the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:12.158361Z",
     "iopub.status.busy": "2023-11-18T17:20:12.157411Z",
     "iopub.status.idle": "2023-11-18T17:20:12.165821Z",
     "shell.execute_reply": "2023-11-18T17:20:12.164860Z",
     "shell.execute_reply.started": "2023-11-18T17:20:12.158321Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adam optimizer is chosen, with 1e-3 learning rate\n",
    "model_LEARNING_RATE = 1e-3\n",
    "model_OPTIMIZER = tf.keras.optimizers.Adam(model_LEARNING_RATE)\n",
    "\n",
    "#Loss function is binary crossentropy, as we are dealing with a binary tast\n",
    "model_LOSS = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "#We want to use early stopping, so we set a reasonable value for patience \n",
    "model_EARLY_STOPPING_PATIENCE = 20\n",
    "\n",
    "#We set a high number of max epochs\n",
    "model_MAX_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the benchmark described in `Transfer_learning_study.ipynb`, we use the ConvNeXtXLarge supernet, initialized with the imagenet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:14.882174Z",
     "iopub.status.busy": "2023-11-18T17:20:14.881217Z",
     "iopub.status.idle": "2023-11-18T17:20:24.468948Z",
     "shell.execute_reply": "2023-11-18T17:20:24.467847Z",
     "shell.execute_reply.started": "2023-11-18T17:20:14.882135Z"
    }
   },
   "outputs": [],
   "source": [
    "model_convnetxlarge = tf.keras.applications.convnext.ConvNeXtXLarge(\n",
    "    input_shape = IMG_SHAPE,\n",
    "    include_top = False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "#We freeze the model so that it will not change its weights in this phase\n",
    "model_convnetxlarge.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used anti-overfitting techniques such as dropout and L1L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:24.471242Z",
     "iopub.status.busy": "2023-11-18T17:20:24.470932Z",
     "iopub.status.idle": "2023-11-18T17:20:27.030564Z",
     "shell.execute_reply": "2023-11-18T17:20:27.029680Z",
     "shell.execute_reply.started": "2023-11-18T17:20:24.471215Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.Input(shape=IMG_SHAPE, name=\"input_layer\"),\n",
    "    \n",
    "    #Adding the augmentation\n",
    "    data_augmentation,\n",
    "    \n",
    "    #We introduce input preprocessing\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.convnext.preprocess_input, \n",
    "                           name='convnetxlarge_preprocessing'),\n",
    "    \n",
    "    #Introducing our pretrained model with frozen weights\n",
    "    model_convnetxlarge,\n",
    "    \n",
    "    #We introduce a flatten layer and a dropout layer from the FEN to the classification layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    \n",
    "    #Now we introduce\n",
    "    tf.keras.layers.Dense(1024, activation='relu', \n",
    "                          kernel_regularizer=tf.keras.regularizers.L1L2(1e-3), \n",
    "                          kernel_initializer=tfk.initializers.HeUniform(seed)),\n",
    "    \n",
    "    #tf.keras.layers.Dropout(0.5), #performs better during fine tuning if we remove it\n",
    "\n",
    "    tf.keras.layers.Dense(512, activation='relu', \n",
    "                          kernel_regularizer=tf.keras.regularizers.L1L2(1e-3), \n",
    "                          kernel_initializer=tfk.initializers.HeUniform(seed)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    #We include a final dense layer with just one neuron which is a sigmoid\n",
    "    tf.keras.layers.Dense(1, \n",
    "                          activation='sigmoid', \n",
    "                          kernel_initializer=tf.keras.initializers.GlorotUniform(seed), \n",
    "                          name='output_layer')\n",
    "], \n",
    "                          name = \"ConvNeXtXLarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:27.032258Z",
     "iopub.status.busy": "2023-11-18T17:20:27.031892Z",
     "iopub.status.idle": "2023-11-18T17:20:27.101769Z",
     "shell.execute_reply": "2023-11-18T17:20:27.099708Z",
     "shell.execute_reply.started": "2023-11-18T17:20:27.032208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 19400705 (74.01 MB)\n",
      "Non-trainable params: 348147968 (1.30 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=model_LOSS, optimizer=model_OPTIMIZER, metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without fine tuning, with FEN layers freezed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                        mode='auto', \n",
    "                                                        restore_best_weights=True,\n",
    "                                                        patience=model_EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "model_history = model.fit(\n",
    "  training_set,\n",
    "  validation_data = validation_set,\n",
    "  epochs = model_MAX_EPOCHS, \n",
    "  class_weight = class_weights_dict,\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After this step, we move to the fine tuning, to modify the preassigned weights from ConvNeXtXLarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, while we keep the binary crossentropy loss, we reduce the optimizer fine tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:07.284145Z",
     "iopub.status.busy": "2023-11-18T17:23:07.283160Z",
     "iopub.status.idle": "2023-11-18T17:23:07.292484Z",
     "shell.execute_reply": "2023-11-18T17:23:07.291384Z",
     "shell.execute_reply.started": "2023-11-18T17:23:07.284100Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning_LOSS = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "fine_tuning_LEARNING_RATE = 5.2e-5\n",
    "fine_tuning_OPTIMIZER = tf.keras.optimizers.Adam(fine_tuning_LEARNING_RATE)\n",
    "\n",
    "#Here we basically unfreeze all the layers at a time, \n",
    "#as this will unfreeze everything if the numberof layers is greater \n",
    "fine_tuning_UNFREEZE = 1000 \n",
    "\n",
    "#Again, we introduce the early stopping and the patience to ensure the model trains enough\n",
    "fine_tuning_MAX_EPOCHS = 200\n",
    "fine_tuning_EARLY_STOPPING_PATIENCE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:10.043881Z",
     "iopub.status.busy": "2023-11-18T17:23:10.043117Z",
     "iopub.status.idle": "2023-11-18T17:23:11.711017Z",
     "shell.execute_reply": "2023-11-18T17:23:11.709970Z",
     "shell.execute_reply.started": "2023-11-18T17:23:10.043846Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning_model = model\n",
    "\n",
    "fine_tuning_model.compile(\n",
    "    optimizer=fine_tuning_OPTIMIZER,\n",
    "    loss=fine_tuning_LOSS,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#We start by setting the original model weights\n",
    "fine_tuning_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:12.811374Z",
     "iopub.status.busy": "2023-11-18T17:23:12.810478Z",
     "iopub.status.idle": "2023-11-18T17:23:12.870225Z",
     "shell.execute_reply": "2023-11-18T17:23:12.869311Z",
     "shell.execute_reply.started": "2023-11-18T17:23:12.811333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 19400705 (74.01 MB)\n",
      "Non-trainable params: 348147968 (1.30 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#We print the model summary\n",
    "fine_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we unfreeze the whole model, with the exception of batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:15.150727Z",
     "iopub.status.busy": "2023-11-18T17:23:15.150374Z",
     "iopub.status.idle": "2023-11-18T17:23:15.309648Z",
     "shell.execute_reply": "2023-11-18T17:23:15.308599Z",
     "shell.execute_reply.started": "2023-11-18T17:23:15.150700Z"
    }
   },
   "outputs": [],
   "source": [
    "#We start by setting the weights as trainable\n",
    "fine_tuning_model.get_layer(model_convnetxlarge.name).trainable = True\n",
    "\n",
    "unfreezed = 0\n",
    "\n",
    "#Here we unfreeze all the layers except for the batch normalization ones\n",
    "for i in reversed(range(len(fine_tuning_model.get_layer(model_convnetxlarge.name).layers))):\n",
    "    layer = fine_tuning_model.get_layer(model_convnetxlarge.name).layers[i]\n",
    "    if unfreezed < fine_tuning_UNFREEZE and not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        unfreezed += 1\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:17.219302Z",
     "iopub.status.busy": "2023-11-18T17:23:17.218516Z",
     "iopub.status.idle": "2023-11-18T17:23:17.279285Z",
     "shell.execute_reply": "2023-11-18T17:23:17.278301Z",
     "shell.execute_reply.started": "2023-11-18T17:23:17.219261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 367548673 (1.37 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:44.080476Z",
     "iopub.status.busy": "2023-11-18T17:23:44.080076Z",
     "iopub.status.idle": "2023-11-18T17:23:44.101764Z",
     "shell.execute_reply": "2023-11-18T17:23:44.100833Z",
     "shell.execute_reply.started": "2023-11-18T17:23:44.080441Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning_LEARNING_RATE = 5.2e-5\n",
    "fine_tuning_OPTIMIZER = tf.keras.optimizers.Adam(fine_tuning_LEARNING_RATE)\n",
    "\n",
    "fine_tuning_model.compile(\n",
    "    optimizer=fine_tuning_OPTIMIZER,\n",
    "    loss=fine_tuning_LOSS,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:58.623066Z",
     "iopub.status.busy": "2023-11-18T17:23:58.622666Z",
     "iopub.status.idle": "2023-11-18T17:23:58.684208Z",
     "shell.execute_reply": "2023-11-18T17:23:58.683327Z",
     "shell.execute_reply.started": "2023-11-18T17:23:58.623033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 367548673 (1.37 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:24:01.007485Z",
     "iopub.status.busy": "2023-11-18T17:24:01.006747Z",
     "iopub.status.idle": "2023-11-18T17:24:01.014554Z",
     "shell.execute_reply": "2023-11-18T17:24:01.013339Z",
     "shell.execute_reply.started": "2023-11-18T17:24:01.007446Z"
    }
   },
   "outputs": [],
   "source": [
    "#Implementing early stopping also for fine tuning\n",
    "\n",
    "fine_tuning_total_epochs =  early_stopping.best_epoch + fine_tuning_MAX_EPOCHS\n",
    "\n",
    "fine_tuning_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                              mode='auto', \n",
    "                                                              patience=fine_tuning_EARLY_STOPPING_PATIENCE, \n",
    "                                                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_tuning_model_history = fine_tuning_model.fit(\n",
    "  training_set,\n",
    "    validation_data=validation_set,\n",
    "    class_weight = class_weights_dict,\n",
    "    epochs=fine_tuning_MAX_EPOCHS,\n",
    "    initial_epoch=early_stopping.best_epoch,\n",
    "  callbacks = [fine_tuning_early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "fine_tuning_model.save('SubmissionModel')\n",
    "shutil.make_archive('SubmissionModel', 'zip', 'SubmissionModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'ft_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3991565,
     "sourceId": 6950022,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4009252,
     "sourceId": 6977052,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
