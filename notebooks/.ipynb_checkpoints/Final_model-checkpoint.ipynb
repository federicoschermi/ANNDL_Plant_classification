{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.14\n",
      "  Downloading tensorflow-2.14.0-cp39-cp39-macosx_10_15_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (1.6.3)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow==2.14)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow==2.14) (1.42.0)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14)\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14)\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14)\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.14) (0.35.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.14)\n",
      "  Downloading grpcio-1.59.3-cp39-cp39-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.22.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow==2.14)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (1.26.18)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow==2.14) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.2.2)\n",
      "Downloading tensorflow-2.14.0-cp39-cp39-macosx_10_15_x86_64.whl (229.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.59.3-cp39-cp39-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: flatbuffers, tensorflow-estimator, tensorboard-data-server, keras, grpcio, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.10.0\n",
      "    Uninstalling tensorflow-estimator-2.10.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.4\n",
      "    Uninstalling google-auth-oauthlib-0.4.4:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.4\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.10.0\n",
      "    Uninstalling tensorboard-2.10.0:\n",
      "      Successfully uninstalled tensorboard-2.10.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.10.0\n",
      "    Uninstalling tensorflow-2.10.0:\n",
      "      Successfully uninstalled tensorflow-2.10.0\n",
      "Successfully installed flatbuffers-23.5.26 google-auth-oauthlib-1.0.0 grpcio-1.59.3 keras-2.14.0 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-estimator-2.14.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: keras-cv in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (0.6.4)\n",
      "Requirement already satisfied: packaging in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-cv) (23.1)\n",
      "Requirement already satisfied: absl-py in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-cv) (1.4.0)\n",
      "Requirement already satisfied: regex in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-cv) (2023.10.3)\n",
      "Requirement already satisfied: tensorflow-datasets in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-cv) (4.9.3)\n",
      "Requirement already satisfied: keras-core in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-cv) (0.1.7)\n",
      "Requirement already satisfied: numpy in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-core->keras-cv) (1.24.3)\n",
      "Requirement already satisfied: rich in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-core->keras-cv) (13.7.0)\n",
      "Requirement already satisfied: namex in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-core->keras-cv) (0.0.7)\n",
      "Requirement already satisfied: h5py in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-core->keras-cv) (3.9.0)\n",
      "Requirement already satisfied: dm-tree in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from keras-core->keras-cv) (0.1.8)\n",
      "Requirement already satisfied: array-record in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (0.4.1)\n",
      "Requirement already satisfied: click in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
      "Requirement already satisfied: etils>=0.9.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (1.5.2)\n",
      "Requirement already satisfied: promise in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (2.1.0)\n",
      "Requirement already satisfied: toml in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-datasets->keras-cv) (1.14.1)\n",
      "Requirement already satisfied: fsspec in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (2023.10.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (6.1.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (4.7.1)\n",
      "Requirement already satisfied: zipp in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from promise->tensorflow-datasets->keras-cv) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from rich->keras-core->keras-cv) (2.15.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.61.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/fl.hi1/opt/anaconda3/envs/ENV_NN/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.14\n",
    "!pip install keras-cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:09:50.611616Z",
     "iopub.status.busy": "2023-11-18T17:09:50.611288Z",
     "iopub.status.idle": "2023-11-18T17:09:57.121368Z",
     "shell.execute_reply": "2023-11-18T17:09:57.120315Z",
     "shell.execute_reply.started": "2023-11-18T17:09:50.611586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-11-18 17:09:51.696145: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-18 17:09:51.696216: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-18 17:09:51.696280: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "#General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from shutil import copyfile, make_archive, unpack_archive #for saving the submission\n",
    "import sklearn as scikit_learn\n",
    "from datetime import datetime\n",
    "\n",
    "#For random seed\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "#Check tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:09:57.123433Z",
     "iopub.status.busy": "2023-11-18T17:09:57.122908Z",
     "iopub.status.idle": "2023-11-18T17:09:57.128691Z",
     "shell.execute_reply": "2023-11-18T17:09:57.127859Z",
     "shell.execute_reply.started": "2023-11-18T17:09:57.123404Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'training_data_final' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting general simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:04.378330Z",
     "iopub.status.busy": "2023-11-18T17:10:04.377511Z",
     "iopub.status.idle": "2023-11-18T17:10:04.385202Z",
     "shell.execute_reply": "2023-11-18T17:10:04.384050Z",
     "shell.execute_reply.started": "2023-11-18T17:10:04.378292Z"
    }
   },
   "outputs": [],
   "source": [
    "#Image parameters\n",
    "IMG_SIZE = (96,96)\n",
    "IMG_SHAPE = (96,96,3)\n",
    "\n",
    "#Number of classes: healthy and unhealthy\n",
    "N_CLASSES = 2\n",
    "\n",
    "#We opt for a 80-20 train-validation split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "#We set a large batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Set the random seed for generalizability\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiating dataset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:06.444408Z",
     "iopub.status.busy": "2023-11-18T17:10:06.444036Z",
     "iopub.status.idle": "2023-11-18T17:10:15.533268Z",
     "shell.execute_reply": "2023-11-18T17:10:15.532246Z",
     "shell.execute_reply.started": "2023-11-18T17:10:06.444375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5004 files belonging to 2 classes.\n",
      "Using 4004 files for training.\n",
      "Found 5004 files belonging to 2 classes.\n",
      "Using 1000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    class_names = None,\n",
    "    color_mode='rgb',\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    class_names = None,\n",
    "    color_mode='rgb',\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of images: 5004.\n",
    "#### Training images: 4004.\n",
    "#### Validation images: 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of class weights\n",
    "\n",
    "#### We introduce class weights, that will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:37.211294Z",
     "iopub.status.busy": "2023-11-18T17:10:37.210519Z",
     "iopub.status.idle": "2023-11-18T17:10:37.370801Z",
     "shell.execute_reply": "2023-11-18T17:10:37.369850Z",
     "shell.execute_reply.started": "2023-11-18T17:10:37.211251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5004 images belonging to 2 classes.\n",
      "{0: 0.8068365043534343, 1: 1.3147661586967945}\n"
     ]
    }
   ],
   "source": [
    "#We instantiate a generator to access the data which are stored in two separate folders\n",
    "datagen_for_class_weights = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "my_data = datagen_for_class_weights.flow_from_directory(DATA_DIR)\n",
    "\n",
    "#We count the unique number of occurrences of the class\n",
    "unique = np.unique(my_data.classes, return_counts=True)\n",
    "\n",
    "#We use the scikit_learn function to compute class weights to make the class balanced (inversely proportional to the number of elements per class)\n",
    "myclass_weights = scikit_learn.utils.compute_class_weight('balanced', \n",
    "                                                          y=my_data.classes,\n",
    "                                                          classes = np.unique(my_data.classes) )\n",
    "class_weights_dict = dict(enumerate(myclass_weights.flatten()))\n",
    "\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on our approach, the class weights are approaximately 0.807, 1: 1.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce mixed precision policy to optimize memory consumption\n",
    "\n",
    "``keras.mixed_precision`` allows to se adaptively different precisions (in our case 0.16 and 0.32 float) during training, in a way that optimized memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:39.862587Z",
     "iopub.status.busy": "2023-11-18T17:10:39.861549Z",
     "iopub.status.idle": "2023-11-18T17:10:39.870581Z",
     "shell.execute_reply": "2023-11-18T17:10:39.869404Z",
     "shell.execute_reply.started": "2023-11-18T17:10:39.862540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision is enabled.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':\n",
    "    print(\"Mixed precision is enabled.\")\n",
    "else:\n",
    "    print(\"Mixed precision is not enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce autotune for the training and validation set cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce the autotune function, which is used o optimally tune data. In this case, we are using it to shuffle the elements of the dataset with a buffer size of 1000. \n",
    "\n",
    "More in detail, `training_set.cache()` is used to cache the elements of the dataset. This step is done to improve the performance of data loading by keeping the data in memory. The further arguent introduces a shuffle, which is introduced to  shuffles the elements of the dataset with a buffer size of 1000 to ensure different samples are seen during each epoch. Similar reasoning can be used for `validation_set.cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:42.240165Z",
     "iopub.status.busy": "2023-11-18T17:10:42.239777Z",
     "iopub.status.idle": "2023-11-18T17:10:42.252857Z",
     "shell.execute_reply": "2023-11-18T17:10:42.252060Z",
     "shell.execute_reply.started": "2023-11-18T17:10:42.240133Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "training_set = training_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_set = validation_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the generalizability of our results, we rely on a data augmentation strategy. \n",
    "\n",
    "We introduce a series of basic transformations. For more information on the transformations which we have decided to apply. Please refer to the `augmentation_study.ipynb` notebook for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:10:45.106914Z",
     "iopub.status.busy": "2023-11-18T17:10:45.106535Z",
     "iopub.status.idle": "2023-11-18T17:10:45.131794Z",
     "shell.execute_reply": "2023-11-18T17:10:45.130556Z",
     "shell.execute_reply.started": "2023-11-18T17:10:45.106881Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),  # Horizontal and vertical random flip \n",
    "  tf.keras.layers.RandomBrightness(0.1),  # Random brightness shift until 10% of the intensity\n",
    "  tf.keras.layers.RandomTranslation(\n",
    "      height_factor=0.05,  # Random height shift until 5% of the height\n",
    "      width_factor=0.05,   # Random width shift until 5% of the width\n",
    "      fill_mode='reflect' # when there are holes, the closest pixels are reflected \n",
    "  ),\n",
    "tf.keras.layers.RandomRotation(0.125),  # Random rotation of +-45 degrees\n",
    "], name='data_augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model with ConvNtXLarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we start by setting the hyperparameters specific to the the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:12.158361Z",
     "iopub.status.busy": "2023-11-18T17:20:12.157411Z",
     "iopub.status.idle": "2023-11-18T17:20:12.165821Z",
     "shell.execute_reply": "2023-11-18T17:20:12.164860Z",
     "shell.execute_reply.started": "2023-11-18T17:20:12.158321Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adam optimizer is chosen, with 1e-3 learning rate\n",
    "model_LEARNING_RATE = 1e-3\n",
    "model_OPTIMIZER = tf.keras.optimizers.Adam(model_LEARNING_RATE)\n",
    "\n",
    "\n",
    "#Loss function is binary crossentropy, as we are dealing with a binary tast\n",
    "model_LOSS = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "\n",
    "#We want to use early stopping, so we set a reasonable value for patience \n",
    "model_EARLY_STOPPING_PATIENCE = 20\n",
    "\n",
    "#We set a high number of max epochs\n",
    "model_MAX_EPOCHS = 200\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the benchmark described in `Transfer-learning-study.ipynb`, we use the ConvNeXtXLarge supernet, initialized with the imagenet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:14.882174Z",
     "iopub.status.busy": "2023-11-18T17:20:14.881217Z",
     "iopub.status.idle": "2023-11-18T17:20:24.468948Z",
     "shell.execute_reply": "2023-11-18T17:20:24.467847Z",
     "shell.execute_reply.started": "2023-11-18T17:20:14.882135Z"
    }
   },
   "outputs": [],
   "source": [
    "model_convnetxlarge = tf.keras.applications.convnext.ConvNeXtXLarge(\n",
    "    input_shape = IMG_SHAPE,\n",
    "    include_top = False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "#We freeze the model so that it will not change its weights in this phase\n",
    "model_convnetxlarge.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used anti-overfitting techniques such as dropout and L1L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:24.471242Z",
     "iopub.status.busy": "2023-11-18T17:20:24.470932Z",
     "iopub.status.idle": "2023-11-18T17:20:27.030564Z",
     "shell.execute_reply": "2023-11-18T17:20:27.029680Z",
     "shell.execute_reply.started": "2023-11-18T17:20:24.471215Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.Input(shape=IMG_SHAPE, name=\"input_layer\"),\n",
    "    \n",
    "    #Adding the augmentation\n",
    "    data_augmentation,\n",
    "    \n",
    "    #We introduce input preprocessing\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.convnext.preprocess_input, \n",
    "                           name='convnetxlarge_preprocessing'),\n",
    "    \n",
    "    #Introducing our pretrained model with frozen weights\n",
    "    model_convnetxlarge,\n",
    "    \n",
    "    #We introduce a flatten layer and a dropout layer from the FEN to the classification layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    \n",
    "    #Now we introduce\n",
    "    tf.keras.layers.Dense(1024, activation='relu', \n",
    "                          kernel_regularizer=tf.keras.regularizers.L1L2(1e-3), \n",
    "                          kernel_initializer=tfk.initializers.HeUniform(seed)),\n",
    "    \n",
    "    #tf.keras.layers.Dropout(0.5), #performs better during fine tuning if we remove it\n",
    "\n",
    "    tf.keras.layers.Dense(512, activation='relu', \n",
    "                          kernel_regularizer=tf.keras.regularizers.L1L2(1e-3), \n",
    "                          kernel_initializer=tfk.initializers.HeUniform(seed)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    #We include a final dense layer with just one neuron which is a sigmoid\n",
    "    tf.keras.layers.Dense(1, \n",
    "                          activation='sigmoid', \n",
    "                          kernel_initializer=tf.keras.initializers.GlorotUniform(seed), \n",
    "                          name='output_layer')\n",
    "], \n",
    "                          name = \"ConvNeXtXLarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:20:27.032258Z",
     "iopub.status.busy": "2023-11-18T17:20:27.031892Z",
     "iopub.status.idle": "2023-11-18T17:20:27.101769Z",
     "shell.execute_reply": "2023-11-18T17:20:27.099708Z",
     "shell.execute_reply.started": "2023-11-18T17:20:27.032208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 19400705 (74.01 MB)\n",
      "Non-trainable params: 348147968 (1.30 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=model_LOSS, optimizer=model_OPTIMIZER, metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainiing without fine tuning, with FEN layers freezed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                        mode='auto', \n",
    "                                                        restore_best_weights=True,\n",
    "                                                        patience=model_EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "model_history = model.fit(\n",
    "  training_set,\n",
    "  validation_data = validation_set,\n",
    "  epochs = 3, ###change back to max epochs\n",
    "  class_weight = class_weights_dict,\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After this step, we move to the fine tuning, to modify the preassigned weights from ConvNetXLarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, while we keep the binary crossentropy loss, we reduce the optimizer fine tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:07.284145Z",
     "iopub.status.busy": "2023-11-18T17:23:07.283160Z",
     "iopub.status.idle": "2023-11-18T17:23:07.292484Z",
     "shell.execute_reply": "2023-11-18T17:23:07.291384Z",
     "shell.execute_reply.started": "2023-11-18T17:23:07.284100Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning_LOSS = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "fine_tuning_LEARNING_RATE = 5.2e-5\n",
    "fine_tuning_OPTIMIZER = tf.keras.optimizers.Adam(fine_tuning_LEARNING_RATE)\n",
    "\n",
    "#Here we basically unfreeze all the layers at a time, \n",
    "#as this will unfreeze everything if the numberof layers is greater \n",
    "fine_tuning_UNFREEZE = 1000 \n",
    "\n",
    "#Again, we introduce the early stopping and the patience to ensure the model trains enough\n",
    "model_MAX_EPOCHS = 200\n",
    "fine_tuning_EARLY_STOPPING_PATIENCE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:10.043881Z",
     "iopub.status.busy": "2023-11-18T17:23:10.043117Z",
     "iopub.status.idle": "2023-11-18T17:23:11.711017Z",
     "shell.execute_reply": "2023-11-18T17:23:11.709970Z",
     "shell.execute_reply.started": "2023-11-18T17:23:10.043846Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning_model = model\n",
    "\n",
    "fine_tuning_model.compile(\n",
    "    optimizer=fine_tuning_OPTIMIZER,\n",
    "    loss=fine_tuning_LOSS,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#We start by setting the original model weights\n",
    "fine_tuning_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:12.811374Z",
     "iopub.status.busy": "2023-11-18T17:23:12.810478Z",
     "iopub.status.idle": "2023-11-18T17:23:12.870225Z",
     "shell.execute_reply": "2023-11-18T17:23:12.869311Z",
     "shell.execute_reply.started": "2023-11-18T17:23:12.811333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 19400705 (74.01 MB)\n",
      "Non-trainable params: 348147968 (1.30 GB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#We print the model summary\n",
    "fine_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we unfreeze the whole model, with the exception of batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:15.150727Z",
     "iopub.status.busy": "2023-11-18T17:23:15.150374Z",
     "iopub.status.idle": "2023-11-18T17:23:15.309648Z",
     "shell.execute_reply": "2023-11-18T17:23:15.308599Z",
     "shell.execute_reply.started": "2023-11-18T17:23:15.150700Z"
    }
   },
   "outputs": [],
   "source": [
    "#We start by setting the weights as trainable\n",
    "fine_tuning_model.get_layer(model_convnetxlarge.name).trainable = True\n",
    "\n",
    "unfreezed = 0\n",
    "\n",
    "#Here we unfreeze all the layers except for the batch normalization ones\n",
    "for i in reversed(range(len(fine_tuning_model.get_layer(model_convnetxlarge.name).layers))):\n",
    "    layer = fine_tuning_model.get_layer(model_convnetxlarge.name).layers[i]\n",
    "    if unfreezed < fine_tuning_UNFREEZE and not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        unfreezed += 1\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:17.219302Z",
     "iopub.status.busy": "2023-11-18T17:23:17.218516Z",
     "iopub.status.idle": "2023-11-18T17:23:17.279285Z",
     "shell.execute_reply": "2023-11-18T17:23:17.278301Z",
     "shell.execute_reply.started": "2023-11-18T17:23:17.219261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 367548673 (1.37 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:44.080476Z",
     "iopub.status.busy": "2023-11-18T17:23:44.080076Z",
     "iopub.status.idle": "2023-11-18T17:23:44.101764Z",
     "shell.execute_reply": "2023-11-18T17:23:44.100833Z",
     "shell.execute_reply.started": "2023-11-18T17:23:44.080441Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning_LEARNING_RATE = 5.2e-5\n",
    "fine_tuning_OPTIMIZER = tf.keras.optimizers.Adam(fine_tuning_LEARNING_RATE)\n",
    "\n",
    "fine_tuning_model.compile(\n",
    "    optimizer=fine_tuning_OPTIMIZER,\n",
    "    loss=fine_tuning_LOSS,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:23:58.623066Z",
     "iopub.status.busy": "2023-11-18T17:23:58.622666Z",
     "iopub.status.idle": "2023-11-18T17:23:58.684208Z",
     "shell.execute_reply": "2023-11-18T17:23:58.683327Z",
     "shell.execute_reply.started": "2023-11-18T17:23:58.623033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ConvNeXtXLarge\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " data_augmentation (Sequent  (None, 96, 96, 3)         0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " convnetxlarge_preprocessin  (None, 96, 96, 3)         0         \n",
      " g (Lambda)                                                      \n",
      "                                                                 \n",
      " convnext_xlarge (Functiona  (None, 3, 3, 2048)        348147968 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              18875392  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 367548673 (1.37 GB)\n",
      "Trainable params: 367548673 (1.37 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T17:24:01.007485Z",
     "iopub.status.busy": "2023-11-18T17:24:01.006747Z",
     "iopub.status.idle": "2023-11-18T17:24:01.014554Z",
     "shell.execute_reply": "2023-11-18T17:24:01.013339Z",
     "shell.execute_reply.started": "2023-11-18T17:24:01.007446Z"
    }
   },
   "outputs": [],
   "source": [
    "#Implementing early stopping also for fine tuning\n",
    "\n",
    "fine_tuning_total_epochs =  early_stopping.best_epoch + fine_tuning_MAX_EPOCHS\n",
    "\n",
    "fine_tuning_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                              mode='auto', \n",
    "                                                              patience=fine_tuning_EARLY_STOPPING_PATIENCE, \n",
    "                                                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_tuning_model_history = fine_tuning_model.fit(\n",
    "  training_set,\n",
    "    validation_data=validation_set,\n",
    "    class_weight = class_weights_dict,\n",
    "    epochs=fine_tuning_MAX_EPOCHS,\n",
    "    initial_epoch=early_stopping.best_epoch,\n",
    "  callbacks = [fine_tuning_early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "fine_tuning_model.save('SubmissionModel')\n",
    "shutil.make_archive('SubmissionModel', 'zip', 'SubmissionModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'ft_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3991565,
     "sourceId": 6950022,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4009252,
     "sourceId": 6977052,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
